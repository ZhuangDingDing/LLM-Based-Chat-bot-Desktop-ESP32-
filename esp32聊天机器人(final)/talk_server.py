import socket
import io
import json
import asyncio
import tempfile
import requests
import time
import speech_recognition as sr
from pydub import AudioSegment
import edge_tts

HOST = "0.0.0.0"
PORT = 8888
CHUNK_SIZE = 4096
OLLAMA_URL = "http://localhost:11434/api/generate"

recognizer = sr.Recognizer()
history = []
exit_keywords = ["å†è§", "æ‹œæ‹œ", "é€€å‡º", "bye"]

# æ„é€ å¤§æ¨¡å‹æç¤ºè¯é…ç½®
def build_config(history, user_input):
    prompt = ""
    for turn in history:
        prompt += f"ç”¨æˆ·ï¼š{turn['user']}\nAIï¼š{turn['assistant']}\n"
    prompt += f"ç”¨æˆ·ï¼š{user_input}\nAIï¼š"
    return {
        "model": "llama3:latest",
        "prompt": prompt,
        "temperature": 0.7,
        "max_tokens": 120,
        "top_p": 0.9,
        "top_k": 50,
        "repeat_penalty": 1.1,
        "stream": True,
        "system": "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œè¯·ç”¨ç®€ä½“ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œä¸è¦é‡å¤ç”¨æˆ·çš„æé—®ï¼Œå›ç­”ç®€æ´ä¸€ç‚¹ã€‚"
    }

# Edge TTS åŒæ­¥è¯­éŸ³åˆæˆ
async def synthesize_edge_tts(text, filename="output.wav"):
    communicate = edge_tts.Communicate(text, voice="zh-CN-XiaoxiaoNeural")
    await communicate.save(filename)

def synthesize_tts(text):
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
        asyncio.run(synthesize_edge_tts(text, f.name))
        # ğŸ”„ è½¬æ¢ä¸º 16kHz å•å£°é“ 16bit
        audio = AudioSegment.from_file(f.name)
        audio = audio.set_frame_rate(16000).set_channels(1).set_sample_width(2)

        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as converted:
            audio.export(converted.name, format="wav")
            with open(converted.name, "rb") as af:
                wav_data = af.read()
                print(f"ğŸ”‰ TTS è½¬æ¢åç”Ÿæˆ {len(wav_data)} å­—èŠ‚")
                if len(wav_data) < 1000:
                    print("âš ï¸ TTS åˆæˆå¤±è´¥ï¼Œç”Ÿæˆæ–‡ä»¶ä¸ºç©ºï¼")
                    wav_data = b"\x00" * 16000
                return b"READY_TTS\n" + wav_data + b"<<TTS_END>>"

# ä½¿ç”¨ SpeechRecognition åšè¯­éŸ³è¯†åˆ«
def recognize_from_pcm(pcm_data):
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as f:
            audio = AudioSegment(data=pcm_data, sample_width=2, frame_rate=16000, channels=1)
            audio.export(f.name, format="wav")
            with sr.AudioFile(f.name) as source:
                audio_data = recognizer.record(source)
                text = recognizer.recognize_google(audio_data, language="zh-CN,en-US")
                return text
    except Exception as e:
        print("è¯†åˆ«é”™è¯¯ï¼š", e)
        return ""

def call_ollama(prompt):
    config = build_config(history, prompt)
    response = requests.post(OLLAMA_URL, json=config, stream=True)
    reply = ""
    for line in response.iter_lines():
        if line:
            data = json.loads(line.decode("utf-8"))
            if "response" in data:
                reply += data["response"]
    return reply

def run_server():
    print(f"ğŸŸ¢ æ­£åœ¨ç›‘å¬ç«¯å£ {PORT} ç­‰å¾… ESP32 è¿æ¥...")
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as server:
        server.bind((HOST, PORT))
        server.listen(1)
        conn, addr = server.accept()
        print("âœ… ESP32 å·²è¿æ¥ï¼š", addr)

        try:
            buffer = b""
            while True:
                print("ğŸ§ æ¥æ”¶éŸ³é¢‘æ•°æ®ä¸­...")
                audio_data = bytearray()

                while True:
                    chunk = conn.recv(CHUNK_SIZE)
                    if not chunk:
                        return
                    buffer += chunk
                    if b"<<END>>" in buffer:
                        split_index = buffer.index(b"<<END>>")
                        audio_data = buffer[:split_index]
                        buffer = buffer[split_index + len(b"<<END>>"):]
                        break

                with open("debug_received.wav", "wb") as f:
                    f.write(audio_data)
                print("âœ… å·²ä¿å­˜ä¸€æ®µéŸ³é¢‘ç”¨äºè°ƒè¯•")

                print("ğŸ“¦ æ”¶åˆ°éŸ³é¢‘æ•°æ®ï¼Œå¼€å§‹è¯†åˆ«...")
                user_text = recognize_from_pcm(audio_data)
                print("ğŸ—£ï¸ ç”¨æˆ·è¯´ï¼š", user_text)

                if not user_text.strip():
                    print("âš ï¸ æ— æœ‰æ•ˆè¯­éŸ³å†…å®¹ï¼Œè·³è¿‡å›ç­”ç”Ÿæˆã€‚")
                    conn.sendall(b"READY_TTS\n<<TTS_END>>")
                    continue
                if any(keyword in user_text.lower() for keyword in exit_keywords):
                    print("ğŸ‘‹ å†è§å•¦")
                    audio_reply = synthesize_tts("å†è§å•¦")
                    conn.sendall(audio_reply)
                    conn.sendall(b"READY_TTS\n<<TTS_END>>")
                    break
                
                print("ğŸ¤– è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”...")
                reply = call_ollama(user_text)
                print("ğŸ’¬ AI å›å¤ï¼š", reply)

                audio_reply = synthesize_tts(reply)
                history.append({"user": user_text, "assistant": reply})

                conn.sendall(audio_reply)

        finally:
            conn.close()
            print("ğŸ”Œ è¿æ¥å…³é—­")

if __name__ == "__main__":
    run_server()